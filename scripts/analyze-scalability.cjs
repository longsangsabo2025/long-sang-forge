/**
 * SCALABILITY ANALYSIS: pgvector Knowledge Search
 * ================================================
 * PhÃ¢n tÃ­ch performance khi scale lÃªn hÃ ng triá»‡u documents
 */

require("dotenv").config();
const { Client } = require("pg");

const c = new Client({
  connectionString: process.env.DATABASE_URL,
  ssl: { rejectUnauthorized: false },
});

async function analyze() {
  await c.connect();

  console.log("=".repeat(70));
  console.log("SCALABILITY ANALYSIS: pgvector Knowledge Search");
  console.log("=".repeat(70));

  // 1. Current state
  console.log("\nðŸ“Š CURRENT STATE");
  console.log("-".repeat(70));

  const stats = await c.query(`
    SELECT
      COUNT(*) as total_rows,
      COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as with_embeddings,
      pg_size_pretty(pg_total_relation_size('brain_knowledge')) as table_size,
      pg_size_pretty(pg_relation_size('brain_knowledge')) as data_size
    FROM brain_knowledge
  `);
  console.log("Total rows:", stats.rows[0].total_rows);
  console.log("With embeddings:", stats.rows[0].with_embeddings);
  console.log("Table size:", stats.rows[0].table_size);
  console.log("Data size:", stats.rows[0].data_size);

  // 2. Index analysis
  console.log("\nðŸ“Š INDEX ANALYSIS");
  console.log("-".repeat(70));

  const indexes = await c.query(`
    SELECT indexname, indexdef, pg_size_pretty(pg_relation_size(indexname::regclass)) as size
    FROM pg_indexes
    WHERE tablename = 'brain_knowledge'
  `);
  indexes.rows.forEach((idx) => {
    console.log(`\n${idx.indexname}:`);
    console.log(`  Size: ${idx.size}`);
    console.log(`  Def: ${idx.indexdef.substring(0, 100)}...`);
  });

  // 3. Vector column analysis
  console.log("\nðŸ“Š VECTOR STORAGE ANALYSIS");
  console.log("-".repeat(70));

  const vectorSize = await c.query(`
    SELECT pg_column_size(embedding) as single_vector_bytes
    FROM brain_knowledge
    WHERE embedding IS NOT NULL
    LIMIT 1
  `);
  const avgContent = await c.query(`
    SELECT AVG(LENGTH(content)) as avg_content_length
    FROM brain_knowledge
    WHERE embedding IS NOT NULL
  `);
  const singleVectorBytes = vectorSize.rows[0]?.single_vector_bytes || 0;
  console.log(
    `Single vector(1536) size: ${singleVectorBytes} bytes (~${(singleVectorBytes / 1024).toFixed(
      1
    )} KB)`
  );
  console.log(
    `Avg content length: ${Math.round(avgContent.rows[0]?.avg_content_length || 0)} chars`
  );

  // 4. Projection at scale
  console.log("\n" + "=".repeat(70));
  console.log("ðŸ“ˆ PROJECTION AT SCALE");
  console.log("=".repeat(70));

  const projections = [
    { docs: 1000, label: "1K docs (small)" },
    { docs: 10000, label: "10K docs (medium)" },
    { docs: 100000, label: "100K docs (large)" },
    { docs: 1000000, label: "1M docs (enterprise)" },
  ];

  console.log(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scale               â”‚ Vector Size  â”‚ Index Size*  â”‚ Query Time** â”‚ Monthly Cost â”‚
â”‚                     â”‚ (raw data)   â”‚ (HNSW est.)  â”‚ (estimated)  â”‚ (Supabase)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤`);

  projections.forEach((p) => {
    const vectorMB = ((p.docs * singleVectorBytes) / 1024 / 1024).toFixed(1);
    const indexMB = ((p.docs * singleVectorBytes * 1.5) / 1024 / 1024).toFixed(1); // HNSW ~1.5x data

    // Query time estimation (logarithmic with HNSW)
    let queryTime;
    if (p.docs <= 1000) queryTime = "~50ms";
    else if (p.docs <= 10000) queryTime = "~100ms";
    else if (p.docs <= 100000) queryTime = "~200ms";
    else queryTime = "~500ms";

    // Supabase cost (rough estimate based on storage + compute)
    let cost;
    const totalGB = (parseFloat(vectorMB) + parseFloat(indexMB)) / 1024;
    if (totalGB < 8) cost = "$25/mo (Pro)";
    else if (totalGB < 50) cost = "$75/mo";
    else cost = "$200+/mo";

    console.log(
      `â”‚ ${p.label.padEnd(19)} â”‚ ${(vectorMB + " MB").padStart(12)} â”‚ ${(indexMB + " MB").padStart(
        12
      )} â”‚ ${queryTime.padStart(12)} â”‚ ${cost.padStart(12)} â”‚`
    );
  });

  console.log(`â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* HNSW index typically 1.5x raw vector data
** With proper indexing (HNSW, ef_search=100)
`);

  // 5. TEXT vs VECTOR comparison at scale
  console.log("\n" + "=".repeat(70));
  console.log("âš–ï¸  TEXT vs VECTOR PARAMETER - SCALABILITY COMPARISON");
  console.log("=".repeat(70));

  console.log(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Factor                  â”‚ VECTOR Parameter        â”‚ TEXT Parameter          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage Impact          â”‚ None                    â”‚ None                    â”‚
â”‚                         â”‚ (same DB storage)       â”‚ (same DB storage)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Network Bandwidth       â”‚ ~32KB per query         â”‚ ~32KB per query         â”‚
â”‚                         â”‚ (JSON serialized)       â”‚ (same - both text)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Query Overhead          â”‚ 0ms                     â”‚ ~0.01ms (cast)          â”‚
â”‚ (per query)             â”‚                         â”‚ (negligible)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Index Usage             â”‚ âœ… Direct HNSW          â”‚ âœ… Same (after cast)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ At 1M documents         â”‚ ~500ms query            â”‚ ~500ms query            â”‚
â”‚                         â”‚ (index-dominated)       â”‚ (index-dominated)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PostgREST Stability     â”‚ âš ï¸ Schema cache risk    â”‚ âœ… Always works         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type Safety             â”‚ âœ… Compile-time check   â”‚ âš ï¸ Runtime check        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Code Portability        â”‚ âš ï¸ pgvector specific    â”‚ âœ… Universal JSON       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸŽ¯ KEY INSIGHT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
At scale (100K+ docs), the difference between TEXT and VECTOR is NEGLIGIBLE.

The bottleneck is:
1. Embedding generation: ~500ms (OpenAI API call)
2. Vector search (HNSW): ~100-500ms depending on scale
3. LLM response: ~1500ms

TEXT cast overhead: ~0.01ms = 0.002% of total time

The REAL optimization opportunities at scale:
`);

  // 6. Real optimization recommendations
  console.log(`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸš€ REAL OPTIMIZATION STRATEGIES FOR SCALE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ðŸ“Š USE HNSW INDEX (not IVFFlat)
   Current: No vector index or IVFFlat
   Better: CREATE INDEX ON brain_knowledge USING hnsw (embedding vector_cosine_ops)
   Impact: 10-100x faster queries at scale

2. ðŸ”€ PARTITION BY DOMAIN
   Current: Single table
   Better: Partition by domain_id (má»—i domain cÃ³ knowledge riÃªng)
   Impact: Query chá»‰ scan partition cáº§n thiáº¿t

3. ðŸ’¾ QUANTIZATION (for 1M+ docs)
   halfvec: 50% storage, ~95% accuracy
   binary: 97% storage reduction, ~90% accuracy

4. ðŸ§  EMBEDDING CACHING
   Cache frequent queries â†’ skip OpenAI call (500ms saved!)

5. ðŸŽ¯ DIMENSION REDUCTION
   text-embedding-3-small: 1536 dims
   Can reduce to 512 dims: 66% storage saved, ~97% accuracy

6. ðŸ“¦ CHUNKING STRATEGY
   Better chunks = better retrieval = fewer docs needed

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ’¡ RECOMMENDATION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

For YOUR use case (Long Sang knowledge base):

SHORT TERM (< 10K docs):
  â†’ Keep TEXT param (stable, simple)
  â†’ Add HNSW index

MEDIUM TERM (10K-100K docs):
  â†’ Add HNSW index + partition by domain
  â†’ Consider embedding caching

LONG TERM (100K+ docs):
  â†’ Switch to halfvec (50% storage)
  â†’ Use dedicated vector DB (Pinecone/Weaviate) if needed
  â†’ TEXT vs VECTOR: KHÃ”NG quan trá»ng á»Ÿ scale nÃ y
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
`);

  await c.end();
}

analyze().catch((e) => console.error("Error:", e.message));
